{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6147909f-7b77-4bc2-b662-eb001fe1d7fa",
   "metadata": {},
   "source": [
    "# Intro to NLP: Representations, Embeddings, and Sentiment Classification\n",
    "\n",
    "This notebook explores core NLP representation techniques (one‑hot, bag‑of‑words, TF‑IDF), embeddings/visualizations, and a simple sentiment classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddbbd91-a488-4829-98b3-64f18899c49b",
   "metadata": {},
   "source": [
    "## How to represent text for AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fb975f",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "**What it does:**  \n",
    "Turns each word in a sentence into a simple list of numbers, where only one number is \"on\" (1) and the rest are \"off\" (0). This helps computers understand which words are present, without any math or meaning.\n",
    "\n",
    "**Why it's useful:**  \n",
    "It's a basic way to show computers what words are in a sentence, so they can start to \"see\" text.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Sentence: \"I like pizza\"\n",
    "\n",
    "Vocabulary: [\"i\", \"like\", \"pizza\"]\n",
    "\n",
    "One-hot encoding matrix:\n",
    "\n",
    "| Word   | i | like | pizza |\n",
    "|--------|---|------|-------|\n",
    "| i      | 1 | 0    | 0     |\n",
    "| like   | 0 | 1    | 0     |\n",
    "| pizza  | 0 | 0    | 1     |\n",
    "\n",
    "So, \"i\" becomes [1, 0, 0], \"like\" becomes [0, 1, 0], and \"pizza\" becomes [0, 0, 1].\n",
    "\n",
    "This is how computers turn words into numbers using one-hot encoding!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8c4403",
   "metadata": {},
   "source": [
    "### How Rows and Tokens Work in One-Hot Encoding\n",
    "- Each **row** in the one-hot encoding matrix stands for a word (token) from your sentence, in the order they appear.\n",
    "- For example, if your sentence is:\n",
    "  `Should we go to a pizzeria or do you a prefer a restaurant?`\n",
    "- The tokens will be:\n",
    "  `['should', 'we', 'go', 'to', 'a', 'pizzeria', 'or', 'do', 'you', 'a', 'prefer', 'a', 'restaurant']`\n",
    "- The matrix will have 13 rows (one for each token above).\n",
    "- Each row shows which word from the vocabulary is present at that position in the sentence.\n",
    "\n",
    "**Summary:**\n",
    "- Number of rows = number of tokens (words) in your sentence.\n",
    "- Number of columns = number of unique words (vocabulary) in your sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9062e60c",
   "metadata": {},
   "source": [
    "### What are tokens?\n",
    "- **Tokens** are just the individual words in your sentence, after cleaning (removing punctuation, making lowercase, etc.).\n",
    "- For example, if your sentence is \"Should we go to a pizzeria or do you a prefer a restaurant?\", the tokens will be:\n",
    "  `['should', 'we', 'go', 'to', 'a', 'pizzeria', 'or', 'do', 'you', 'a', 'prefer', 'a', 'restaurant']`\n",
    "- You can see the tokens by adding this line to your code:\n",
    "```python\n",
    "print(\"Tokens:\", tokens)\n",
    "```\n",
    "- This helps you understand how the sentence is split into words before any encoding happens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5f24f8",
   "metadata": {},
   "source": [
    "### Understanding the One-Hot Encoding Output\n",
    "- The output matrix has rows for each word in your sentence (in order) and columns for each unique word in the sentence (the vocabulary).\n",
    "- Each row has a single `1` in the column that matches the word at that position; all other entries are `0`.\n",
    "- This lets the computer know which word is present at each spot, but doesn't tell it anything about meaning or similarity yet.\n",
    "\n",
    "**Vocabulary = Unique Words**\n",
    "- The vocabulary is simply the list of all unique words found in your sentence.\n",
    "- For example, if your sentence is \"Should we go to a pizzeria or do you a prefer a restaurant?\", the vocabulary will be all the different words in that sentence, with no repeats.\n",
    "\n",
    "**Summary:**\n",
    "- One-hot encoding is a way for computers to turn words into numbers, so they can start working with text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b10a40a-3f4b-4711-893b-a96486a43aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bdf0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def clean_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Lowercase, strip punctuation, collapse spaces, optionally remove digits, and split.\n",
    "    Keeps basic ASCII letters; removes punctuation so tokens like 'awesome?' become 'awesome'.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    # Replace non-letters with space (keeps a–z only)\n",
    "    text = re.sub(r\"[^a-z]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e953ef4b-eb91-41dd-befc-6ea5d20f1167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(sentence):\n",
    "    tokens = clean_tokenize(sentence)\n",
    "    vocabulary = sorted(set(tokens))\n",
    "    word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "    one_hot_matrix = np.zeros((len(tokens), len(vocabulary)), dtype=int)\n",
    "    for i, word in enumerate(tokens):\n",
    "        one_hot_matrix[i, word_to_index[word]] = 1\n",
    "    return one_hot_matrix, vocabulary, tokens\n",
    "\n",
    "\n",
    "# Example of usage\n",
    "sentence = \"Should we go to a pizzeria or do you a prefer a restaurant?\"\n",
    "one_hot_matrix, vocabulary, tokens = one_hot_encoding(sentence)\n",
    "\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"Tokens :\", tokens)\n",
    "print(\"Vocabulary is unique ({}) words:\".format(\n",
    "    len(vocabulary)), vocabulary[:20])\n",
    "print(\"One-Hot Encoding Matrix shape:\", one_hot_matrix.shape)\n",
    "print(one_hot_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a563be",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "\n",
    "What it does:\n",
    "Counts how many times each word appears in each sentence (or document). It builds a vocabulary of unique words and then makes a table of counts.\n",
    "\n",
    "Why it’s useful:\n",
    "Lets us compare sentences by the words they contain and how often. Helpful for search, clustering, or as input features for ML models.\n",
    "\n",
    "Tiny example (3 sentences)\n",
    "Sentences:\n",
    "1) \"This movie is awesome awesome\"\n",
    "2) \"I do not say is good, but neither awesome\"\n",
    "3) \"Awesome? Only a fool can say that\"\n",
    "\n",
    "Vocabulary (sorted): [\"a\", \"awesome\", \"but\", \"can\", \"do\", \"fool\", \"good\", \"i\", \"is\", \"movie\", \"neither\", \"not\", \"only\", \"say\", \"that\", \"this\"]\n",
    "\n",
    "Counts matrix (rows = sentences, columns = vocabulary):\n",
    "| sentence | a | awesome | but | can | do | fool | good | i | is | movie | neither | not | only | say | that | this |\n",
    "|---------:|:-:|:------:|:---:|:---:|:--:|:----:|:----:|:-:|:--:|:-----:|:-------:|:---:|:----:|:---:|:----:|:----:|\n",
    "| 1        | 0 |   2    |  0  |  0  | 0  |  0   |  0   | 0 |  1 |   1   |    0    |  0  |  0   |  0  |  0   |  1   |\n",
    "| 2        | 0 |   1    |  1  |  0  | 1  |  0   |  1   | 1 |  1 |   0   |    1    |  1  |  0   |  1  |  0   |  0   |\n",
    "| 3        | 1 |   1    |  0  |  1  | 0  |  1   |  0   | 0 |  0 |   0   |    0    |  0  |  1   |  1  |  1   |  0   |\n",
    "\n",
    "How to read it\n",
    "- Row 1 has \"awesome\" twice, and contains the words \"is\", \"movie\", and \"this\" once each.\n",
    "- Row 2 includes many single‑occurrence words like \"but\", \"do\", \"good\", \"i\", \"is\", \"neither\", \"not\", \"say\", and one \"awesome\".\n",
    "- Row 3 contains words like \"a\", \"awesome\", \"can\", \"fool\", \"only\", \"say\", \"that\" each once.\n",
    "\n",
    "Notes\n",
    "- Bag of Words ignores order: \"movie awesome\" equals \"awesome movie\".\n",
    "- It captures counts, not meaning. Later, TF‑IDF and embeddings improve on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3434e6-e8a5-4a28-a8a3-aef8fa9e3539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(sentences):\n",
    "    \"\"\"\n",
    "    Creates a bag-of-words representation of a list of documents.\n",
    "    \"\"\"\n",
    "    tokenized_sentences = [clean_tokenize(sentence) for sentence in sentences]\n",
    "    flat_words = [word for sublist in tokenized_sentences for word in sublist]\n",
    "    vocabulary = sorted(set(flat_words))\n",
    "    word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "\n",
    "    bow_matrix = np.zeros((len(sentences), len(vocabulary)), dtype=int)\n",
    "    for i, sentence in enumerate(tokenized_sentences):\n",
    "        for word in sentence:\n",
    "            if word in word_to_index:\n",
    "                bow_matrix[i, word_to_index[word]] += 1\n",
    "    return vocabulary, bow_matrix\n",
    "\n",
    "\n",
    "# Example of usage\n",
    "corpus = [\n",
    "    \"This movie is awesome awesome\",\n",
    "    \"I do not say is good, but neither awesome\",\n",
    "    \"Awesome? Only a fool can say that\"\n",
    "]\n",
    "vocabulary, bow_matrix = bag_of_words(corpus)\n",
    "print(\"Vocabulary ({}):\".format(len(vocabulary)), vocabulary[:20])\n",
    "print(\"Bag of Words Matrix shape:\", bow_matrix.shape)\n",
    "print(bow_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3129aa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: view Bag of Words as a labeled table\n",
    "import pandas as pd\n",
    "bow_df = pd.DataFrame(bow_matrix, columns=vocabulary)\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13e71a8",
   "metadata": {},
   "source": [
    "## Term Frequency (TF) and TF-IDF\n",
    "\n",
    "**What it does:**  \n",
    "- **TF:** Shows how often each word appears in a sentence, compared to the total number of words.\n",
    "- **TF-IDF:** Adjusts these counts to highlight words that are special or unique to each sentence, and downplays words that appear everywhere.\n",
    "\n",
    "**Why it's useful:**  \n",
    "TF-IDF helps computers find the most important words in your text, which is great for searching, sorting, or understanding meaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30509f63",
   "metadata": {},
   "source": [
    "### How to read TF and TF‑IDF\n",
    "\n",
    "- Term Frequency (TF): Within one sentence, count each word and divide by total words in that sentence. TF values are between 0 and 1.\n",
    "- Inverse Document Frequency (IDF): Words that appear in many sentences get lower weight; rare words get higher weight.\n",
    "- TF‑IDF = TF × IDF: High when a word is frequent in a sentence but rare across other sentences.\n",
    "\n",
    "Tiny example\n",
    "Sentences:\n",
    "1) \"this movie is awesome awesome\"\n",
    "2) \"i do not say is good, but neither awesome\"\n",
    "3) \"awesome? only a fool can say that\"\n",
    "\n",
    "Intuition:\n",
    "- The word \"awesome\" appears in all three sentences, so its IDF weight is lower than a word that appears in only one sentence.\n",
    "- A word that appears many times in a single sentence (like \"awesome\" in sentence 1) can still get a decent score, but very common words across sentences (like \"is\") get pushed down.\n",
    "\n",
    "Reading the printed matrix:\n",
    "- Rows are sentences; columns are words in the vocabulary.\n",
    "- Larger numbers indicate words that are more important to that sentence.\n",
    "- Compare columns: a word with small values in many rows is probably not very informative; a word with a large value in a single row is likely distinctive there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbe37ba",
   "metadata": {},
   "source": [
    "#### Tiny numeric walk‑through\n",
    "\n",
    "Let’s compute TF and IDF for the word \"awesome\" across the three sentences above.\n",
    "\n",
    "1) Token counts\n",
    "- Sentence 1: this(1), movie(1), is(1), awesome(2) → total words = 5\n",
    "- Sentence 2: i(1), do(1), not(1), say(1), is(1), good(1), but(1), neither(1), awesome(1) → total words = 9\n",
    "- Sentence 3: a(1), awesome(1), can(1), fool(1), only(1), say(1), that(1) → total words = 7\n",
    "\n",
    "2) TF (per sentence)\n",
    "- TF1(awesome) = 2 / 5 = 0.40\n",
    "- TF2(awesome) = 1 / 9 ≈ 0.11\n",
    "- TF3(awesome) = 1 / 7 ≈ 0.14\n",
    "\n",
    "3) IDF (with smoothing like in code): idf = log((1 + N) / (1 + df)) + 1\n",
    "- N = 3 sentences; df(awesome) = 3 (appears in all three)\n",
    "- IDF(awesome) = log((1+3)/(1+3)) + 1 = log(1) + 1 = 1.0\n",
    "\n",
    "4) TF‑IDF = TF × IDF\n",
    "- TF‑IDF1(awesome) = 0.40 × 1.0 = 0.40\n",
    "- TF‑IDF2(awesome) ≈ 0.11 × 1.0 ≈ 0.11\n",
    "- TF‑IDF3(awesome) ≈ 0.14 × 1.0 ≈ 0.14\n",
    "\n",
    "Interpretation\n",
    "- \"awesome\" is still most important to sentence 1 due to repetition.\n",
    "- Words rare across sentences (df close to 1) would get higher IDF and stand out in their sentence; words present everywhere get down‑weighted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7243950a",
   "metadata": {},
   "source": [
    "#### Tiny TF table (normalized counts)\n",
    "\n",
    "Using the same three sentences:\n",
    "1) \"this movie is awesome awesome\" (5 words)\n",
    "2) \"i do not say is good, but neither awesome\" (9 words)\n",
    "3) \"awesome? only a fool can say that\" (7 words)\n",
    "\n",
    "Vocabulary (sorted): [\"a\", \"awesome\", \"but\", \"can\", \"do\", \"fool\", \"good\", \"i\", \"is\", \"movie\", \"neither\", \"not\", \"only\", \"say\", \"that\", \"this\"]\n",
    "\n",
    "TF matrix (rows = sentences, columns = vocabulary; values rounded to 2 decimals):\n",
    "| sentence | a | awesome | but | can | do | fool | good | i | is | movie | neither | not | only | say | that | this |\n",
    "|---------:|:-:|:------:|:---:|:---:|:--:|:----:|:----:|:-:|:--:|:-----:|:-------:|:---:|:----:|:---:|:----:|:----:|\n",
    "| 1        |0.00|  0.40  |0.00 |0.00 |0.00| 0.00 |0.00  |0.00|0.20| 0.20 | 0.00    |0.00 |0.00 |0.00 |0.00 |0.20 |\n",
    "| 2        |0.00|  0.11  |0.11 |0.00 |0.11| 0.00 |0.11  |0.11|0.11| 0.00 | 0.11    |0.11 |0.00 |0.11 |0.00 |0.00 |\n",
    "| 3        |0.14|  0.14  |0.00 |0.14 |0.00| 0.14 |0.00  |0.00|0.00| 0.00 | 0.00    |0.00 |0.14 |0.14 |0.14 |0.00 |\n",
    "\n",
    "Notes\n",
    "- Each row sums to 1.0 (up to rounding), because TF divides counts by total words in that sentence.\n",
    "- TF emphasizes repetition within a sentence; IDF will reduce the weight of words spread across many sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66dbeda-eef2-4cbe-885e-72cd1bc6da8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(sentences):\n",
    "    \"\"\"Compute the term frequency matrix for a list of sentences.\"\"\"\n",
    "    tokenized = [clean_tokenize(s) for s in sentences]\n",
    "    vocabulary = sorted(set(w for sent in tokenized for w in sent))\n",
    "    word_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "    tf = np.zeros((len(sentences), len(vocabulary)), dtype=np.float32)\n",
    "    for i, words in enumerate(tokenized):\n",
    "        word_count = len(words) if len(words) > 0 else 1\n",
    "        for word in words:\n",
    "            tf[i, word_index[word]] += 1 / word_count\n",
    "    return tf, vocabulary\n",
    "\n",
    "\n",
    "def compute_idf(sentences, vocabulary):\n",
    "    \"\"\"Compute the inverse document frequency with sklearn-style smoothing.\"\"\"\n",
    "    tokenized = [set(clean_tokenize(s)) for s in sentences]\n",
    "    num_documents = len(sentences)\n",
    "    idf = np.zeros(len(vocabulary), dtype=np.float32)\n",
    "    word_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "    for word in vocabulary:\n",
    "        df = sum(1 for sent in tokenized if word in sent)\n",
    "        # sklearn-style: log((1 + N) / (1 + df)) + 1 → equals 1.0 for df == N\n",
    "        idf[word_index[word]] = np.log((1 + num_documents) / (1 + df)) + 1.0\n",
    "    return idf\n",
    "\n",
    "\n",
    "def tf_idf(sentences):\n",
    "    \"\"\"Generate a TF-IDF matrix for a list of sentences.\"\"\"\n",
    "    tf, vocabulary = compute_tf(sentences)\n",
    "    idf = compute_idf(sentences, vocabulary)\n",
    "    tf_idf_matrix = tf * idf\n",
    "    return vocabulary, tf_idf_matrix\n",
    "\n",
    "\n",
    "vocabulary, tf_idf_matrix = tf_idf(corpus)\n",
    "print(\"Vocabulary ({}):\".format(len(vocabulary)), vocabulary[:20])\n",
    "print(\"TF-IDF Matrix shape:\", tf_idf_matrix.shape)\n",
    "print(tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: view TF / IDF / TF-IDF as tables\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "tf_matrix, tf_vocab = compute_tf(corpus)\n",
    "idf_vector = compute_idf(corpus, tf_vocab)\n",
    "\n",
    "\n",
    "tf_df = pd.DataFrame(tf_matrix, columns=tf_vocab)\n",
    "idf_df = pd.DataFrame([idf_vector], columns=tf_vocab)\n",
    "tfidf_df = pd.DataFrame(tf_idf_matrix, columns=vocabulary)\n",
    "\n",
    "print(\"TF table:\")\n",
    "display(tf_df.round(2))\n",
    "print(\"IDF vector:\")\n",
    "display(idf_df.round(2))\n",
    "print(\"TF-IDF table:\")\n",
    "display(tfidf_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0240bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top TF-IDF terms per sentence\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def top_tfidf_per_doc(vocab, tfidf_matrix, top_k=3):\n",
    "    for i in range(tfidf_matrix.shape[0]):\n",
    "        row = tfidf_matrix[i]\n",
    "        idxs = np.argsort(-row)[:top_k]\n",
    "        pairs = [(vocab[j], float(row[j])) for j in idxs if row[j] > 0]\n",
    "        print(f\"Sentence {i+1}: {pairs}\")\n",
    "\n",
    "\n",
    "top_tfidf_per_doc(vocabulary, tf_idf_matrix, top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a190e6-707d-4354-974a-0196e1309c4c",
   "metadata": {},
   "source": [
    "## Embeddings: Making Words Understandable for Computers\n",
    "\n",
    "**What it does:**  \n",
    "Turns words into lists of numbers (called \"embeddings\") so computers can compare words and find patterns. These numbers capture some meaning and relationships between words.\n",
    "\n",
    "**Why it's useful:**  \n",
    "Embeddings help computers understand that words like \"good\" and \"great\" are similar, while \"good\" and \"bad\" are different. This is a big step up from just counting words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a7d984f-da22-4ab5-a498-31430b94fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from adjustText import adjust_text\n",
    "from umap import UMAP\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539e71a5-12cd-438e-b6a0-96741a2dd36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this for unzip and read the file NON-WINDOWS\n",
    "!wget https://github.com/SalvatoreRa/tutorial/blob/main/datasets/IMDB.zip?raw=true\n",
    "!unzip IMDB.zip?raw=true\n",
    "df=pd.read_csv(\"IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9018a3d",
   "metadata": {},
   "source": [
    "Windows users: Skip the shell cell above and run the next cell instead (pure-Python download/extract)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c4c7383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "9406b7c6-b034-4aff-879b-8078037ac7dd",
       "rows": [
        [
         "0",
         "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.",
         "positive"
        ],
        [
         "1",
         "A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional 'dream' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell's murals decorating every surface) are terribly well done.",
         "positive"
        ],
        [
         "2",
         "I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.<br /><br />This was the most I'd laughed at one of Woody's comedies in years (dare I say a decade?). While I've never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.",
         "positive"
        ],
        [
         "3",
         "Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.",
         "negative"
        ],
        [
         "4",
         "Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a variation on the Arthur Schnitzler's play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.<br /><br />The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case with most of the people we encounter.<br /><br />The acting is good under Mr. Mattei's direction. Steve Buscemi, Rosario Dawson, Carol Kane, Michael Imperioli, Adrian Grenier, and the rest of the talented cast, make these characters come alive.<br /><br />We wish Mr. Mattei good luck and await anxiously for his next work.",
         "positive"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download and extract IMDB.zip using Python (Windows-friendly). No external tools needed.\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def ensure_imdb_csv(csv_name=\"IMDB Dataset.csv\"):\n",
    "    if os.path.exists(csv_name):\n",
    "        return csv_name\n",
    "\n",
    "    zip_url = \"https://raw.githubusercontent.com/SalvatoreRa/tutorial/main/datasets/IMDB.zip\"\n",
    "    zip_path = \"IMDB.zip\"\n",
    "\n",
    "    print(f\"Downloading {zip_url} -> {zip_path} ...\")\n",
    "    urllib.request.urlretrieve(zip_url, zip_path)\n",
    "\n",
    "    print(f\"Extracting {zip_path} ...\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "        zf.extractall(\".\")\n",
    "\n",
    "    try:\n",
    "        os.remove(zip_path)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    if not os.path.exists(csv_name):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Expected '{csv_name}' after extraction, but it was not found.\")\n",
    "    return csv_name\n",
    "\n",
    "\n",
    "csv_path = ensure_imdb_csv()\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3217f190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NLTK punkt tokenizer...\n",
      "NLTK punkt tokenizer is available.\n"
     ]
    }
   ],
   "source": [
    "# Ensure NLTK punkt is available (needed for word_tokenize)\n",
    "import nltk\n",
    "try:\n",
    "    print(\"Checking for NLTK punkt tokenizer...\")\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"NLTK punkt tokenizer is available.\")\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK punkt tokenizer...\")\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f29b27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\PK.Viswanatha-\n",
      "[nltk_data]     Swamy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "# Ensure NLTK punkt_tab is available (needed by newer NLTK for sentence tokenization)\n",
    "import nltk\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    try:\n",
    "        nltk.download('punkt_tab')\n",
    "    except Exception:\n",
    "        local_dir = str(Path.cwd() / 'nltk_data')\n",
    "        os.makedirs(local_dir, exist_ok=True)\n",
    "        nltk.download('punkt_tab', download_dir=local_dir)\n",
    "        if local_dir not in nltk.data.path:\n",
    "            nltk.data.path.append(local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9181f172-4593-47a8-88ae-49983d4711bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:02<00:00, 16891.12it/s]\n",
      "100%|██████████| 50000/50000 [00:02<00:00, 16891.12it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "reviews_processed",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tokens",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "bec96769-a4a7-4e62-9189-7866b42ad904",
       "rows": [
        [
         "0",
         "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.",
         "positive",
         "one of the other reviewers has mentioned that after watching just oz episode youll be hooked they are right as this is exactly what happened with methe first thing that struck me about oz was its brutality and unflinching scenes of violence which set in right from the word go trust me this is not show for the faint hearted or timid this show pulls no punches with regards to drugs sex or violence its is hardcore in the classic use of the wordit is called oz as that is the nickname given to the oswald maximum security state penitentary it focuses mainly on emerald city an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda em city is home to manyaryans muslims gangstas latinos christians italians irish and moreso scuffles death stares dodgy dealings and shady agreements are never far awayi would say the main appeal of the show is due to the fact that it goes where other shows wouldnt dare forget pretty pictures painted for mainstream audiences forget charm forget romanceoz doesnt mess around the first episode ever saw struck me as so nasty it was surreal couldnt say was ready for it but as watched more developed taste for oz and got accustomed to the high levels of graphic violence not just violence but injustice crooked guards wholl be sold out for nickel inmates wholl kill on order and get away with it well mannered middle class inmates being turned into prison bitches due to their lack of street skills or prison experience watching oz you may become comfortable with what is uncomfortable viewingthats if you can get in touch with your darker side",
         "['one', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', 'oz', 'episode', 'youll', 'be', 'hooked', 'they', 'are', 'right', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'methe', 'first', 'thing', 'that', 'struck', 'me', 'about', 'oz', 'was', 'its', 'brutality', 'and', 'unflinching', 'scenes', 'of', 'violence', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go', 'trust', 'me', 'this', 'is', 'not', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid', 'this', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', 'sex', 'or', 'violence', 'its', 'is', 'hardcore', 'in', 'the', 'classic', 'use', 'of', 'the', 'wordit', 'is', 'called', 'oz', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'it', 'focuses', 'mainly', 'on', 'emerald', 'city', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', 'em', 'city', 'is', 'home', 'to', 'manyaryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'and', 'moreso', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'awayi', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'wouldnt', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romanceoz', 'doesnt', 'mess', 'around', 'the', 'first', 'episode', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', 'couldnt', 'say', 'was', 'ready', 'for', 'it', 'but', 'as', 'watched', 'more', 'developed', 'taste', 'for', 'oz', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', 'not', 'just', 'violence', 'but', 'injustice', 'crooked', 'guards', 'wholl', 'be', 'sold', 'out', 'for', 'nickel', 'inmates', 'wholl', 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', 'well', 'mannered', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', 'watching', 'oz', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewingthats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side']"
        ],
        [
         "1",
         "A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional 'dream' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell's murals decorating every surface) are terribly well done.",
         "positive",
         "wonderful little production the filming technique is very unassuming very oldtimebbc fashion and gives comforting and sometimes discomforting sense of realism to the entire piece the actors are extremely well chosen michael sheen not only has got all the polari but he has all the voices down pat too you can truly see the seamless editing guided by the references to williams diary entries not only is it well worth the watching but it is terrificly written and performed piece masterful production about one of the great masters of comedy and his life the realism really comes home with the little things the fantasy of the guard which rather than use the traditional dream techniques remains solid then disappears it plays on our knowledge and our senses particularly with the scenes concerning orton and halliwell and the sets particularly of their flat with halliwells murals decorating every surface are terribly well done",
         "['wonderful', 'little', 'production', 'the', 'filming', 'technique', 'is', 'very', 'unassuming', 'very', 'oldtimebbc', 'fashion', 'and', 'gives', 'comforting', 'and', 'sometimes', 'discomforting', 'sense', 'of', 'realism', 'to', 'the', 'entire', 'piece', 'the', 'actors', 'are', 'extremely', 'well', 'chosen', 'michael', 'sheen', 'not', 'only', 'has', 'got', 'all', 'the', 'polari', 'but', 'he', 'has', 'all', 'the', 'voices', 'down', 'pat', 'too', 'you', 'can', 'truly', 'see', 'the', 'seamless', 'editing', 'guided', 'by', 'the', 'references', 'to', 'williams', 'diary', 'entries', 'not', 'only', 'is', 'it', 'well', 'worth', 'the', 'watching', 'but', 'it', 'is', 'terrificly', 'written', 'and', 'performed', 'piece', 'masterful', 'production', 'about', 'one', 'of', 'the', 'great', 'masters', 'of', 'comedy', 'and', 'his', 'life', 'the', 'realism', 'really', 'comes', 'home', 'with', 'the', 'little', 'things', 'the', 'fantasy', 'of', 'the', 'guard', 'which', 'rather', 'than', 'use', 'the', 'traditional', 'dream', 'techniques', 'remains', 'solid', 'then', 'disappears', 'it', 'plays', 'on', 'our', 'knowledge', 'and', 'our', 'senses', 'particularly', 'with', 'the', 'scenes', 'concerning', 'orton', 'and', 'halliwell', 'and', 'the', 'sets', 'particularly', 'of', 'their', 'flat', 'with', 'halliwells', 'murals', 'decorating', 'every', 'surface', 'are', 'terribly', 'well', 'done']"
        ],
        [
         "2",
         "I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.<br /><br />This was the most I'd laughed at one of Woody's comedies in years (dare I say a decade?). While I've never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.",
         "positive",
         "thought this was wonderful way to spend time on too hot summer weekend sitting in the air conditioned theater and watching lighthearted comedy the plot is simplistic but the dialogue is witty and the characters are likable even the well bread suspected serial killer while some may be disappointed when they realize this is not match point risk addiction thought it was proof that woody allen is still fully in control of the style many of us have grown to lovethis was the most id laughed at one of woodys comedies in years dare say decade while ive never been impressed with scarlet johanson in this she managed to tone down her sexy image and jumped right into average but spirited young womanthis may not be the crown jewel of his career but it was wittier than devil wears prada and more interesting than superman great comedy to go see with friends",
         "['thought', 'this', 'was', 'wonderful', 'way', 'to', 'spend', 'time', 'on', 'too', 'hot', 'summer', 'weekend', 'sitting', 'in', 'the', 'air', 'conditioned', 'theater', 'and', 'watching', 'lighthearted', 'comedy', 'the', 'plot', 'is', 'simplistic', 'but', 'the', 'dialogue', 'is', 'witty', 'and', 'the', 'characters', 'are', 'likable', 'even', 'the', 'well', 'bread', 'suspected', 'serial', 'killer', 'while', 'some', 'may', 'be', 'disappointed', 'when', 'they', 'realize', 'this', 'is', 'not', 'match', 'point', 'risk', 'addiction', 'thought', 'it', 'was', 'proof', 'that', 'woody', 'allen', 'is', 'still', 'fully', 'in', 'control', 'of', 'the', 'style', 'many', 'of', 'us', 'have', 'grown', 'to', 'lovethis', 'was', 'the', 'most', 'id', 'laughed', 'at', 'one', 'of', 'woodys', 'comedies', 'in', 'years', 'dare', 'say', 'decade', 'while', 'ive', 'never', 'been', 'impressed', 'with', 'scarlet', 'johanson', 'in', 'this', 'she', 'managed', 'to', 'tone', 'down', 'her', 'sexy', 'image', 'and', 'jumped', 'right', 'into', 'average', 'but', 'spirited', 'young', 'womanthis', 'may', 'not', 'be', 'the', 'crown', 'jewel', 'of', 'his', 'career', 'but', 'it', 'was', 'wittier', 'than', 'devil', 'wears', 'prada', 'and', 'more', 'interesting', 'than', 'superman', 'great', 'comedy', 'to', 'go', 'see', 'with', 'friends']"
        ],
        [
         "3",
         "Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.",
         "negative",
         "basically theres family where little boy jake thinks theres zombie in his closet his parents are fighting all the timethis movie is slower than soap opera and suddenly jake decides to become rambo and kill the zombieok first of all when youre going to make film you must decide if its thriller or drama as drama the movie is watchable parents are divorcing arguing like in real life and then we have jake with his closet which totally ruins all the film expected to see boogeyman similar movie and instead watched drama with some meaningless thriller spots out of just for the well playing parents descent dialogs as for the shots with jake just ignore them",
         "['basically', 'theres', 'family', 'where', 'little', 'boy', 'jake', 'thinks', 'theres', 'zombie', 'in', 'his', 'closet', 'his', 'parents', 'are', 'fighting', 'all', 'the', 'timethis', 'movie', 'is', 'slower', 'than', 'soap', 'opera', 'and', 'suddenly', 'jake', 'decides', 'to', 'become', 'rambo', 'and', 'kill', 'the', 'zombieok', 'first', 'of', 'all', 'when', 'youre', 'going', 'to', 'make', 'film', 'you', 'must', 'decide', 'if', 'its', 'thriller', 'or', 'drama', 'as', 'drama', 'the', 'movie', 'is', 'watchable', 'parents', 'are', 'divorcing', 'arguing', 'like', 'in', 'real', 'life', 'and', 'then', 'we', 'have', 'jake', 'with', 'his', 'closet', 'which', 'totally', 'ruins', 'all', 'the', 'film', 'expected', 'to', 'see', 'boogeyman', 'similar', 'movie', 'and', 'instead', 'watched', 'drama', 'with', 'some', 'meaningless', 'thriller', 'spots', 'out', 'of', 'just', 'for', 'the', 'well', 'playing', 'parents', 'descent', 'dialogs', 'as', 'for', 'the', 'shots', 'with', 'jake', 'just', 'ignore', 'them']"
        ],
        [
         "4",
         "Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a variation on the Arthur Schnitzler's play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.<br /><br />The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case with most of the people we encounter.<br /><br />The acting is good under Mr. Mattei's direction. Steve Buscemi, Rosario Dawson, Carol Kane, Michael Imperioli, Adrian Grenier, and the rest of the talented cast, make these characters come alive.<br /><br />We wish Mr. Mattei good luck and await anxiously for his next work.",
         "positive",
         "petter matteis love in the time of money is visually stunning film to watch mr mattei offers us vivid portrait about human relations this is movie that seems to be telling us what money power and success do to people in the different situations we encounter this being variation on the arthur schnitzlers play about the same theme the director transfers the action to the present time new york where all these different characters meet and connect each one is connected in one way or another to the next person but no one seems to know the previous point of contact stylishly the film has sophisticated luxurious look we are taken to see how these people live and the world they live in their own habitatthe only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits big city is not exactly the best place in which human relations find sincere fulfillment as one discerns is the case with most of the people we encounterthe acting is good under mr matteis direction steve buscemi rosario dawson carol kane michael imperioli adrian grenier and the rest of the talented cast make these characters come alivewe wish mr mattei good luck and await anxiously for his next work",
         "['petter', 'matteis', 'love', 'in', 'the', 'time', 'of', 'money', 'is', 'visually', 'stunning', 'film', 'to', 'watch', 'mr', 'mattei', 'offers', 'us', 'vivid', 'portrait', 'about', 'human', 'relations', 'this', 'is', 'movie', 'that', 'seems', 'to', 'be', 'telling', 'us', 'what', 'money', 'power', 'and', 'success', 'do', 'to', 'people', 'in', 'the', 'different', 'situations', 'we', 'encounter', 'this', 'being', 'variation', 'on', 'the', 'arthur', 'schnitzlers', 'play', 'about', 'the', 'same', 'theme', 'the', 'director', 'transfers', 'the', 'action', 'to', 'the', 'present', 'time', 'new', 'york', 'where', 'all', 'these', 'different', 'characters', 'meet', 'and', 'connect', 'each', 'one', 'is', 'connected', 'in', 'one', 'way', 'or', 'another', 'to', 'the', 'next', 'person', 'but', 'no', 'one', 'seems', 'to', 'know', 'the', 'previous', 'point', 'of', 'contact', 'stylishly', 'the', 'film', 'has', 'sophisticated', 'luxurious', 'look', 'we', 'are', 'taken', 'to', 'see', 'how', 'these', 'people', 'live', 'and', 'the', 'world', 'they', 'live', 'in', 'their', 'own', 'habitatthe', 'only', 'thing', 'one', 'gets', 'out', 'of', 'all', 'these', 'souls', 'in', 'the', 'picture', 'is', 'the', 'different', 'stages', 'of', 'loneliness', 'each', 'one', 'inhabits', 'big', 'city', 'is', 'not', 'exactly', 'the', 'best', 'place', 'in', 'which', 'human', 'relations', 'find', 'sincere', 'fulfillment', 'as', 'one', 'discerns', 'is', 'the', 'case', 'with', 'most', 'of', 'the', 'people', 'we', 'encounterthe', 'acting', 'is', 'good', 'under', 'mr', 'matteis', 'direction', 'steve', 'buscemi', 'rosario', 'dawson', 'carol', 'kane', 'michael', 'imperioli', 'adrian', 'grenier', 'and', 'the', 'rest', 'of', 'the', 'talented', 'cast', 'make', 'these', 'characters', 'come', 'alivewe', 'wish', 'mr', 'mattei', 'good', 'luck', 'and', 'await', 'anxiously', 'for', 'his', 'next', 'work']"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>reviews_processed</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>[one, of, the, other, reviewers, has, mentione...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful little production the filming techni...</td>\n",
       "      <td>[wonderful, little, production, the, filming, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thought this was wonderful way to spend time o...</td>\n",
       "      <td>[thought, this, was, wonderful, way, to, spend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basically theres family where little boy jake ...</td>\n",
       "      <td>[basically, theres, family, where, little, boy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter matteis love in the time of money is vi...</td>\n",
       "      <td>[petter, matteis, love, in, the, time, of, mon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                   reviews_processed  \\\n",
       "0  one of the other reviewers has mentioned that ...   \n",
       "1  wonderful little production the filming techni...   \n",
       "2  thought this was wonderful way to spend time o...   \n",
       "3  basically theres family where little boy jake ...   \n",
       "4  petter matteis love in the time of money is vi...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [one, of, the, other, reviewers, has, mentione...  \n",
       "1  [wonderful, little, production, the, filming, ...  \n",
       "2  [thought, this, was, wonderful, way, to, spend...  \n",
       "3  [basically, theres, family, where, little, boy...  \n",
       "4  [petter, matteis, love, in, the, time, of, mon...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocessing_reviews(reviews):\n",
    "    \"\"\"\n",
    "    simple preprocessing: splitting on the space and remove word less than 1 chr\n",
    "    \"\"\"\n",
    "\n",
    "    processed_reviews = []\n",
    "\n",
    "    for review in tqdm(reviews):\n",
    "        review = re.sub('<[^>]+>', '', review)\n",
    "        processed = re.sub('[^a-zA-Z ]', '', review)\n",
    "        words = processed.split()\n",
    "        processed_reviews.append(\n",
    "            ' '.join([word.lower() for word in words if len(word) > 1]))\n",
    "    return processed_reviews\n",
    "\n",
    "\n",
    "df['reviews_processed'] = preprocessing_reviews(df['review'])\n",
    "df['tokens'] = df['reviews_processed'].apply(nltk.word_tokenize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9e51014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time needed on subset: 0.18 mins\n"
     ]
    }
   ],
   "source": [
    "# Quick demo: train Word2Vec on a subset to keep runtime short\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "start_time = time.time()\n",
    "# adjust N for speed/quality tradeoff\n",
    "tokens_sample = df['tokens'].head(5000).tolist()\n",
    "model = Word2Vec(sentences=tokens_sample, sg=1,\n",
    "                 vector_size=100, window=5, workers=4)\n",
    "print(f'Time needed on subset: {(time.time() - start_time) / 60:.2f} mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e778594-cc87-4293-9afa-8bc792a6b52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# embedding\n",
    "model = Word2Vec(sentences=df['tokens'].tolist(),\n",
    "                 sg=1,\n",
    "                 vector_size=100,\n",
    "                 window=5,\n",
    "                 workers=4)\n",
    "\n",
    "print(f'Time needed : {(time.time() - start_time) / 60:.2f} mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ab78382d-964f-4fa6-8e8d-6581ac22d771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Entire set of words in the model\n",
    "all_words = list(model.wv.index_to_key)\n",
    "all_vectors = np.array([model.wv[word] for word in all_words])\n",
    "\n",
    "# Highlighted words and their vectors\n",
    "highlight_words = ['Berlin', 'Paris', 'London', 'Rome', 'Italy',\n",
    "                   'France', 'Germany', 'England', 'movie', 'production', 'good', 'bad']\n",
    "highs = [w.lower() for w in highlight_words]\n",
    "indices = [all_words.index(word) for word in highs if word in all_words]\n",
    "highlight_vectors = np.array([all_vectors[index] for index in indices])\n",
    "\n",
    "linked = linkage(highlight_vectors, 'ward')\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "dendrogram(linked,\n",
    "           orientation='top',\n",
    "           labels=highlight_words,\n",
    "           distance_sort='descending',\n",
    "           show_leaf_counts=True)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig('word_dendrogram.jpg', format='jpeg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4d295b-0bf7-4fa7-b844-7fdb5c176cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE to the entire set of vectors\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "Y_tsne = tsne.fit_transform(all_vectors)\n",
    "\n",
    "highlight_words = ['berlin', 'rome', 'London', 'France', 'Germany',\n",
    "                   'movie', 'production', 'mother', 'family']\n",
    "\n",
    "highs = [w.lower() for w in highlight_words]\n",
    "indices = [all_words.index(word) for word in highs if word in all_words]\n",
    "highlight_vectors = np.array([all_vectors[index] for index in indices])\n",
    "Y_highlight = Y_tsne[indices]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "\n",
    "sns.scatterplot(x=Y_tsne[:, 0], y=Y_tsne[:, 1], color=\"lightgrey\", alpha=0.3)\n",
    "\n",
    "# Plot highlighted words\n",
    "palette = sns.color_palette(\"hsv\", len(highlight_words))\n",
    "texts = []\n",
    "for i, word in enumerate(highlight_words):\n",
    "    plt.scatter(Y_highlight[i, 0], Y_highlight[i, 1],\n",
    "                color=palette[i], s=100, label=word)\n",
    "    # adjust text\n",
    "    texts.append(plt.text(Y_highlight[i, 0],\n",
    "                 Y_highlight[i, 1], word, fontsize=12))\n",
    "\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "plt.title('t-SNE visualization of Word2Vec embeddings', fontsize=20)\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend(title='Highlighted Words', title_fontsize='13', fontsize='11')\n",
    "plt.savefig('word_tsne.jpg', format='jpeg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdda4a6-df35-4f03-98c0-5cad29b2b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply UMAP to the entire set of vectors\n",
    "umap = UMAP(n_components=2, random_state=42)\n",
    "Y_umap = umap.fit_transform(all_vectors)\n",
    "\n",
    "Y_highlight = Y_umap[indices]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x=Y_umap[:, 0], y=Y_umap[:, 1], color=\"lightgrey\", alpha=0.3)\n",
    "\n",
    "palette = sns.color_palette(\"hsv\", len(highlight_words))\n",
    "texts = []\n",
    "for i, word in enumerate(highlight_words):\n",
    "    plt.scatter(Y_highlight[i, 0], Y_highlight[i, 1],\n",
    "                color=palette[i], s=100, label=word)\n",
    "\n",
    "    texts.append(plt.text(Y_highlight[i, 0],\n",
    "                 Y_highlight[i, 1], word, fontsize=12))\n",
    "\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "plt.title('UMAP visualization of Word2Vec embeddings', fontsize=20)\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend(title='Highlighted Words', title_fontsize='13', fontsize='11')\n",
    "plt.savefig('word_umap.jpg', format='jpeg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63ba90d-66ba-490f-9b27-666d0656006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vectors_and_angle(v1, v2):\n",
    "\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    cosine_similarity = dot_product / (norm_v1 * norm_v2)\n",
    "    angle_radians = np.arccos(cosine_similarity)\n",
    "    angle_degrees = np.degrees(angle_radians)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    ax.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy',\n",
    "              scale=1, color='r', label=f\"Vector 1: {v1}\")\n",
    "    ax.quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy',\n",
    "              scale=1, color='b', label=f\"Vector 2: {v2}\")\n",
    "\n",
    "    start_angle = np.arctan2(v1[1], v1[0])\n",
    "    if np.cross(v1, v2) < 0:\n",
    "        angle_radians = -angle_radians\n",
    "\n",
    "    theta = np.linspace(start_angle, start_angle + angle_radians, 100)\n",
    "    r = 0.5 * min(np.linalg.norm(v1), np.linalg.norm(v2))\n",
    "    x = r * np.cos(theta)\n",
    "    y = r * np.sin(theta)\n",
    "\n",
    "    ax.plot(x, y, linestyle='-', color='green', lw=2)\n",
    "\n",
    "    midpoint = (start_angle + angle_radians / 2)\n",
    "    ax.annotate(r'$\\theta$', xy=(r * np.cos(midpoint), r * np.sin(midpoint)), xytext=(20, 10),\n",
    "                textcoords='offset points', fontsize=16, arrowprops=dict(arrowstyle='->', lw=0.5))\n",
    "\n",
    "    max_range = np.max(\n",
    "        np.abs(np.vstack([v1, v2, [x.max(), y.max()]]))) * 1.1  # 10% padding\n",
    "    ax.set_xlim([-max_range, max_range])\n",
    "    ax.set_ylim([-max_range, max_range])\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.axhline(0, color='black', linewidth=0.5)\n",
    "    plt.axvline(0, color='black', linewidth=0.5)\n",
    "    plt.title(f'Angle between vectors: {angle_degrees:.2f} degrees')\n",
    "    plt.suptitle(\n",
    "        f'Similarity between vectors: {cosine_similarity:.2f}', fontsize=10, y=.95)\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig('cosine_similarity.jpg', format='jpeg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return cosine_similarity, angle_degrees\n",
    "\n",
    "\n",
    "# Example usage\n",
    "v1 = np.array([2, 3])\n",
    "v2 = np.array([-1, 2])\n",
    "cos_sim, angle = plot_vectors_and_angle(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11858cc1-a482-4bbc-bb1f-deb25448e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_1 = \"good\"\n",
    "syn = \"great\"\n",
    "ant = \"bad\"\n",
    "most_sim = model.wv.most_similar(\"good\")\n",
    "print(\"Top 3 most simalr words to {} are :{}\".format(word_1, most_sim[:3]))\n",
    "\n",
    "synonyms_dist = model.wv.distance(word_1, syn)\n",
    "antonyms_dist = model.wv.distance(word_1, ant)\n",
    "print(\"Synonyms {}, {} have cosine distance: {}\".format(word_1, syn, synonyms_dist))\n",
    "print(\"Antonyms {}, {} have cosine distance: {}\".format(word_1, ant, antonyms_dist))\n",
    "a = 'king'\n",
    "a_star = 'man'\n",
    "b = 'woman'\n",
    "b_star = model.wv.most_similar(positive=[a, b], negative=[a_star])\n",
    "print(\"{} is to {} as {} is to: {} \".format(a, a_star, b, b_star[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e32d319-8ee4-4bff-907a-96e2f9a623fc",
   "metadata": {},
   "source": [
    "# RNN, LSTM, GRU, CNN for Text\n",
    "\n",
    "**What it does:**  \n",
    "These are different types of computer models that can read and understand text. They help computers learn patterns, remember information, and make predictions about words or sentences.\n",
    "\n",
    "**Why it's useful:**  \n",
    "These models are the building blocks for things like chatbots, translators, and tools that can read and understand human language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e3d509-fac6-47f3-8c35-20a5e12c7bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117a8c1d-b7b8-4e4f-b8cc-105ecbdcea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.random.random((10, 5, 3))\n",
    "\n",
    "data_tensor = torch.tensor(array, dtype=torch.float32)\n",
    "RNN = nn.RNN(input_size=3, hidden_size=10,\n",
    "             num_layers=1, batch_first=True)\n",
    "output, hidden = RNN(data_tensor)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644e7d7c-6765-4a60-a4a5-376f17edc665",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor = torch.tensor(np.random.random((10, 5, 3)), dtype=torch.float32)\n",
    "LSTM = nn.LSTM(input_size=3, hidden_size=10,\n",
    "               num_layers=1, batch_first=True)\n",
    "output, (hidden, cell) = LSTM(data_tensor)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a9d409-0a35-418f-989e-30b3ddb2c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor = torch.tensor(np.random.random((10, 5, 3)), dtype=torch.float32)\n",
    "GRU = nn.GRU(input_size=3, hidden_size=10,\n",
    "             num_layers=1, batch_first=True)\n",
    "output, hidden = GRU(data_tensor)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb91f421-e1f9-4fce-a612-af4bbda84ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor = torch.tensor(np.random.random((10, 5, 3)), dtype=torch.float32)\n",
    "Conv1d = nn.Conv1d(in_channels=5, out_channels=16,\n",
    "                   kernel_size=3, stride=1, padding=1)\n",
    "output = Conv1d(data_tensor)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d285e13-1b67-4310-8de8-9523094d3f44",
   "metadata": {},
   "source": [
    "# Classify Reviews with Deep Learning\n",
    "\n",
    "**What it does:**  \n",
    "Uses a computer model to read movie reviews and decide if they are positive or negative. The model learns from lots of examples and gets better over time.\n",
    "\n",
    "**Why it's useful:**  \n",
    "This is how computers can automatically sort reviews, detect spam, or even understand how people feel about products or movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2f9f3e-c998-47f0-ac32-49e0a70167fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Check if we GPU available\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6802968-c646-4e70-a848-cd1a940be983",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this for unzip and read the file\n",
    "try:\n",
    "    df=pd.read_csv(\"IMDB Dataset.csv\")\n",
    "except:\n",
    "    !wget https://github.com/SalvatoreRa/tutorial/blob/main/datasets/IMDB.zip?raw=true\n",
    "    !unzip IMDB.zip?raw=true\n",
    "\n",
    "df['sentiment_encoded'] = np.where(df['sentiment']=='positive',0,1)\n",
    "X,y = df['review'].values, df['sentiment_encoded'].values\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,stratify=y, test_size=.2)\n",
    "x_train,x_val,y_train,y_val = train_test_split(x_train,y_train,stratify=y_train, test_size=.1)\n",
    "y_train, y_val, y_test = np.array(y_train), np.array(y_val), np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eae24a",
   "metadata": {},
   "source": [
    "Windows users: If the shell commands above fail on this platform, run the next cell to download/extract IMDB via Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2815e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows-friendly: download and extract IMDB.zip without external tools\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def ensure_imdb_csv(csv_name=\"IMDB Dataset.csv\", zip_url=\"https://raw.githubusercontent.com/SalvatoreRa/tutorial/main/datasets/IMDB.zip\"):\n",
    "    if os.path.exists(csv_name):\n",
    "        return csv_name\n",
    "    zip_path = \"IMDB.zip\"\n",
    "    print(f\"Downloading {zip_url} -> {zip_path} ...\")\n",
    "    urllib.request.urlretrieve(zip_url, zip_path)\n",
    "    print(f\"Extracting {zip_path} ...\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "        zf.extractall(\".\")\n",
    "    try:\n",
    "        os.remove(zip_path)\n",
    "    except OSError:\n",
    "        pass\n",
    "    if not os.path.exists(csv_name):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Expected '{csv_name}' after extraction, but it was not found.\")\n",
    "    return csv_name\n",
    "\n",
    "\n",
    "csv_path = ensure_imdb_csv()\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "df['sentiment_encoded'] = np.where(df['sentiment'] == 'positive', 0, 1)\n",
    "X, y = df['review'].values, df['sentiment_encoded'].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=.2)\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train, y_train, stratify=y_train, test_size=.1)\n",
    "y_train, y_val, y_test = np.array(y_train), np.array(y_val), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a23ff15-11e3-4dc7-90c5-5bd621dcbf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordclouds(df):\n",
    "    '''\n",
    "    Generate two word clouds from the 50 most frequent words in the list of positive and negative reviews respectively.\n",
    "\n",
    "    '''\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Separating reviews by sentiment\n",
    "    positive_reviews = df[df['sentiment'] == 'positive']['review']\n",
    "    negative_reviews = df[df['sentiment'] == 'negative']['review']\n",
    "\n",
    "    def get_words(reviews):\n",
    "        all_words = []\n",
    "        for review in reviews:\n",
    "            review = re.sub(r\"[^\\w\\s]\", '', review)\n",
    "            review = re.sub(r\"\\d\", '', review)\n",
    "            words = review.split()\n",
    "            filtered_words = [\n",
    "                word for word in words if word not in stop_words and len(word) > 1]\n",
    "            all_words.extend(filtered_words)\n",
    "        return all_words\n",
    "\n",
    "    positive_words = get_words(positive_reviews)\n",
    "    negative_words = get_words(negative_reviews)\n",
    "\n",
    "    positive_counts = Counter(positive_words)\n",
    "    negative_counts = Counter(negative_words)\n",
    "\n",
    "    positive_wordcloud = WordCloud(\n",
    "        width=400,\n",
    "        height=400,\n",
    "        max_words=200,\n",
    "        max_font_size=100,\n",
    "        background_color='white',\n",
    "        color_func=lambda *args, **kwargs: \"green\"\n",
    "    ).generate_from_frequencies(positive_counts)\n",
    "\n",
    "    negative_wordcloud = WordCloud(\n",
    "        width=400,\n",
    "        height=400,\n",
    "        max_words=200,\n",
    "        max_font_size=100,\n",
    "        background_color='white',\n",
    "        color_func=lambda *args, **kwargs: \"red\"\n",
    "    ).generate_from_frequencies(negative_counts)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(positive_wordcloud, interpolation='bilinear')\n",
    "    plt.title('Positive Reviews')\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(negative_wordcloud, interpolation='bilinear')\n",
    "    plt.title('Negative Reviews')\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('word_clouds.jpg', format='jpeg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "generate_wordclouds(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b963cd8-8ba1-46cc-9019-2f05b04166fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_review_length_by_sentiment(df):\n",
    "    '''\n",
    "    Plots histograms of the number of words per review for positive and negative reviews with summary statistics.\n",
    "\n",
    "    '''\n",
    "\n",
    "    positive_reviews = df[df['sentiment'] == 'positive']['review']\n",
    "    negative_reviews = df[df['sentiment'] == 'negative']['review']\n",
    "\n",
    "    def get_review_lengths(reviews):\n",
    "        return [len(review.split()) for review in reviews]\n",
    "\n",
    "    positive_lengths = get_review_lengths(positive_reviews)\n",
    "    negative_lengths = get_review_lengths(negative_reviews)\n",
    "\n",
    "    def get_summary_stats(lengths):\n",
    "        return {\n",
    "            'min': np.min(lengths),\n",
    "            'avg': np.mean(lengths),\n",
    "            'median': np.median(lengths),\n",
    "            'max': np.max(lengths)\n",
    "        }\n",
    "\n",
    "    pos_stats = get_summary_stats(positive_lengths)\n",
    "    neg_stats = get_summary_stats(negative_lengths)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot for positive reviews\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(positive_lengths, bins=30, color='green',\n",
    "             edgecolor='black', alpha=0.7)\n",
    "    plt.title('Word Distribution for Positive Reviews')\n",
    "    plt.xlabel('Number of Words')\n",
    "    plt.ylabel('Number of Reviews')\n",
    "    plt.grid(True)\n",
    "    stats_text = f\"Min: {pos_stats['min']}\\nAvg: {pos_stats['avg']:.2f}\\nMedian: {pos_stats['median']}\\nMax: {pos_stats['max']}\"\n",
    "    plt.text(0.95, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=10, verticalalignment='top',\n",
    "             horizontalalignment='right', bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='wheat', alpha=0.5))\n",
    "\n",
    "    # Plot for negative reviews\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(negative_lengths, bins=30, color='red',\n",
    "             edgecolor='black', alpha=0.7)\n",
    "    plt.title('Word Distribution for Negative Reviews')\n",
    "    plt.xlabel('Number of Words')\n",
    "    plt.ylabel('Number of Reviews')\n",
    "    plt.grid(True)\n",
    "    stats_text = f\"Min: {neg_stats['min']}\\nAvg: {neg_stats['avg']:.2f}\\nMedian: {neg_stats['median']}\\nMax: {neg_stats['max']}\"\n",
    "    plt.text(0.95, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=10, verticalalignment='top',\n",
    "             horizontalalignment='right', bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='wheat', alpha=0.5))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('review_length.jpg', format='jpeg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_review_length_by_sentiment(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a8e8f-2a28-4267-a75e-6d43fa6518c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_review(review):\n",
    "    '''\n",
    "    Cleaning of the review: remove non-alphanumeric characters, collapse whitespace, and remove digits.\n",
    "    '''\n",
    "    review = re.sub(r\"[^\\w\\s]\", ' ',\n",
    "                    review)  # Replace non-word characters with space\n",
    "    # Replace multiple spaces with a single space\n",
    "    review = re.sub(r\"\\s+\", ' ', review)\n",
    "    review = re.sub(r\"\\d\", '', review)        # Remove digits\n",
    "    return review.strip().lower()\n",
    "\n",
    "\n",
    "def tokenize_reviews(x_train, x_val, x_test):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # tokenize and clean list of reviews\n",
    "    def tokenize_and_filter(reviews):\n",
    "        word_list = []\n",
    "        for review in reviews:\n",
    "            words = word_tokenize(preprocess_review(review))\n",
    "            filtered_words = [\n",
    "                word for word in words if word not in stop_words and len(word) > 1]\n",
    "            word_list.extend(filtered_words)\n",
    "        return word_list\n",
    "\n",
    "    # Create a corpus\n",
    "    corpus = Counter(tokenize_and_filter(x_train))\n",
    "    # Select the 1000 most common words\n",
    "    vocab = {word: i+1 for i,\n",
    "             word in enumerate([word for word, freq in corpus.most_common(1000)])}\n",
    "\n",
    "    # convert reviews into sequences of indices\n",
    "    def vectorize_reviews(reviews):\n",
    "        vectorized = []\n",
    "        for review in reviews:\n",
    "            tokenized = word_tokenize(preprocess_review(review))\n",
    "            indexed = [vocab[word] for word in tokenized if word in vocab]\n",
    "            vectorized.append(indexed)\n",
    "        return vectorized\n",
    "\n",
    "    _x_train = vectorize_reviews(x_train)\n",
    "    _x_val = vectorize_reviews(x_val)\n",
    "    _x_test = vectorize_reviews(x_test)\n",
    "\n",
    "    return _x_train, _x_val, _x_test, vocab\n",
    "\n",
    "\n",
    "x_train, x_val, x_test, vocab = tokenize_reviews(x_train, x_val, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0670aa4b-555f-4927-be47-3be9fed927c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_review_length_distribution(tokenized_reviews):\n",
    "    '''\n",
    "    Plots a histogram of the lengths of tokenized reviews and includes a box with summary statistics.\n",
    "\n",
    "    '''\n",
    "\n",
    "    review_lengths = [len(review) for review in tokenized_reviews]\n",
    "\n",
    "    # Calculate summary statistics\n",
    "    min_length = np.min(review_lengths)\n",
    "    avg_length = np.mean(review_lengths)\n",
    "    median_length = np.median(review_lengths)\n",
    "    max_length = np.max(review_lengths)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(review_lengths, bins=30, color='blue',\n",
    "             edgecolor='black', alpha=0.7)\n",
    "    plt.title('Distribution of Review Lengths')\n",
    "    plt.xlabel('Number of Tokens')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "\n",
    "    stats_text = f'Min Length: {min_length}\\nAverage Length: {avg_length:.2f}\\nMedian Length: {median_length}\\nMax Length: {max_length}'\n",
    "    plt.gca().text(0.95, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=10, verticalalignment='top',\n",
    "                   horizontalalignment='right', bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='wheat', alpha=0.5))\n",
    "    plt.savefig('review_length_after_tokenization.jpg',\n",
    "                format='jpeg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_review_length_distribution(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f27e53-73ad-43f2-bf88-4825e376a945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_(reviews, max_seq):\n",
    "    features = np.zeros((len(reviews), max_seq), dtype=int)\n",
    "    for ii, review in enumerate(reviews):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:max_seq]\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(\n",
    "    padding_(x_train, 500)), torch.from_numpy(y_train))\n",
    "valid_data = TensorDataset(torch.from_numpy(\n",
    "    padding_(x_val, 500)), torch.from_numpy(y_val))\n",
    "test_data = TensorDataset(torch.from_numpy(\n",
    "    padding_(x_test, 500)), torch.from_numpy(y_test))\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=50)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=50)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98eb1bd-91c2-47c7-9319-aabe69a04f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, no_layers, vocab_size, hidden_dim, embedding_dim, drop_prob=0.5):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(input_size=embedding_dim, hidden_size=self.hidden_dim,\n",
    "                          num_layers=no_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        embeds = self.embedding(x)\n",
    "        rnn_out, hidden = self.rnn(embeds, hidden)\n",
    "        rnn_out = rnn_out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.dropout(rnn_out)\n",
    "        sig_out = self.sig(out)\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1]\n",
    "        return sig_out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "\n",
    "        h0 = torch.zeros((self.no_layers, batch_size,\n",
    "                         self.hidden_dim)).to(device)\n",
    "        return h0\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "no_layers = 3\n",
    "vocab_size = len(vocab) + 1\n",
    "embedding_dim = 300\n",
    "output_dim = 1\n",
    "hidden_dim = 256\n",
    "\n",
    "# Initialize the model\n",
    "model = SentimentRNN(no_layers, vocab_size, hidden_dim,\n",
    "                     embedding_dim, drop_prob=0.5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9a8a67-45c1-456a-8d2e-6fda0ebd11b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tsne = padding_(x_train, 500)\n",
    "x_train_tsne = x_train_tsne[:1000, :]\n",
    "y_train_tsne = y_train[:1000]\n",
    "\n",
    "\n",
    "def plot_embeddings(x_train, y_train, model, device, batch_size=50):\n",
    "    model.eval()\n",
    "    embeddings_list = []\n",
    "\n",
    "    # Create a DataLoader to handle the x_train data in batches\n",
    "    train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(x_train),\n",
    "                                                   torch.from_numpy(y_train))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        for x_batch, _ in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            hidden = model.init_hidden(x_batch.size(0))\n",
    "\n",
    "            # Feed forward through the model to get to the embeddings layer\n",
    "            embeds = model.embedding(x_batch)\n",
    "            rnn_out, hidden = model.rnn(embeds, hidden)\n",
    "            rnn_out = rnn_out.contiguous().view(-1, model.hidden_dim)  # Flatten the output\n",
    "            out = model.dropout(rnn_out)\n",
    "            linear_output = model.fc(out)\n",
    "\n",
    "            embeddings_list.append(linear_output.cpu())  # Store CPU data\n",
    "\n",
    "    # Concatenate all batch embeddings into a single matrix\n",
    "    all_embeddings = torch.cat(embeddings_list, dim=0)\n",
    "\n",
    "    all_embeddings = all_embeddings.view(-1, 500)\n",
    "\n",
    "    # Reduce dimensions to 2D using t-SNE for visualization\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    embeddings_2d = tsne.fit_transform(all_embeddings.numpy())\n",
    "\n",
    "    df = pd.DataFrame(data=embeddings_2d, columns=['TSNE-1', 'TSNE-2'])\n",
    "    df['label'] = y_train\n",
    "    custom_palette = {0: 'green', 1: 'red'}\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = sns.scatterplot(data=df, x='TSNE-1', y='TSNE-2',\n",
    "                              hue='label', palette=custom_palette, s=60, alpha=0.6)\n",
    "    plt.title('2D t-SNE Visualization of Sentence Embeddings')\n",
    "    plt.xlabel('t-SNE dimension 1')\n",
    "    plt.ylabel('t-SNE dimension 2')\n",
    "    plt.legend(title='Label', bbox_to_anchor=(1.05, 1), loc=2)\n",
    "    plt.savefig('tsne_model_untrained_projection.jpg',\n",
    "                format='jpeg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_embeddings(x_train_tsne, y_train_tsne, model, device, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ed4b6-7677-4918-beb6-3e9a827b0423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "def acc(pred, label):\n",
    "    \"\"\"Calculate accuracy by comparing predicted labels with true labels.\"\"\"\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()\n",
    "\n",
    "\n",
    "clip = 5\n",
    "epochs = 5\n",
    "valid_loss_min = np.inf\n",
    "\n",
    "\n",
    "epoch_tr_loss, epoch_vl_loss = [], []\n",
    "epoch_tr_acc, epoch_vl_acc = [], []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses = []\n",
    "    train_acc = 0.0\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    # Initialize hidden state\n",
    "    h = model.init_hidden(50)\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Detach hidden states\n",
    "        h = h.data\n",
    "\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = acc(output, labels)\n",
    "        train_acc += accuracy\n",
    "\n",
    "        # Clip gradients to prevent exploding gradient issues in RNNs\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    val_losses = []\n",
    "    val_acc = 0.0\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_h = model.init_hidden(50)\n",
    "\n",
    "    for inputs, labels in valid_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Detach hidden states\n",
    "        val_h = val_h.data\n",
    "\n",
    "        output, val_h = model(inputs, val_h)\n",
    "        val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        accuracy = acc(output, labels)\n",
    "        val_acc += accuracy\n",
    "\n",
    "    epoch_train_loss = np.mean(train_losses)\n",
    "    epoch_val_loss = np.mean(val_losses)\n",
    "    epoch_train_acc = train_acc / len(train_loader.dataset)\n",
    "    epoch_val_acc = val_acc / len(valid_loader.dataset)\n",
    "\n",
    "    epoch_tr_loss.append(epoch_train_loss)\n",
    "    epoch_vl_loss.append(epoch_val_loss)\n",
    "    epoch_tr_acc.append(epoch_train_acc)\n",
    "    epoch_vl_acc.append(epoch_val_acc)\n",
    "\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    print(f'Train Loss: {epoch_train_loss} Val Loss: {epoch_val_loss}')\n",
    "    print(\n",
    "        f'Train Accuracy: {epoch_train_acc * 100}% Val Accuracy: {epoch_val_acc * 100}%')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043bc6d4-1b36-4c93-8112-ee4d9cbad9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epoch_tr_acc, label='Train Acc')\n",
    "plt.plot(epoch_vl_acc, label='Validation Acc')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epoch_tr_loss, label='Train loss')\n",
    "plt.plot(epoch_vl_loss, label='Validation loss')\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('accuracy_and_loss.jpg', format='jpeg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b620ca16-a518-458a-9ed5-bd82d7dbef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(model, data_loader, device):\n",
    "    \"\"\"Predict output for a batch of data using the RNN model.\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            batch_size = inputs.size(0)\n",
    "\n",
    "            hidden = model.init_hidden(batch_size).to(device)\n",
    "\n",
    "            output, _ = model(inputs, hidden)\n",
    "\n",
    "            predicted_probs = torch.sigmoid(output)\n",
    "            predicted_labels = (predicted_probs > 0.60).float()\n",
    "\n",
    "            predictions.extend(predicted_labels.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return predictions, true_labels, predicted_probs, labels\n",
    "\n",
    "\n",
    "predictions, true_labels, predicted_probs, labels = predict_batch(\n",
    "    model, test_loader, device)\n",
    "print(f'Accuracy on test set: {accuracy_score(true_labels, predictions)}')\n",
    "# Plot confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\n",
    "            'Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.jpg', format='jpeg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de72f16-2596-4314-a18e-a9d5d776ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings(x_train_tsne, y_train_tsne, model, device, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a00fd-1260-464c-9fdd-db705d273f33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
