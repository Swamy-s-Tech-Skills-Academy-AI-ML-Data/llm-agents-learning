{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00e8c802",
   "metadata": {},
   "source": [
    "# Embeddings & Visualization (Word2Vec, Dendrogram, t‑SNE/UMAP)\n",
    "\n",
    "Make word meanings visible with embeddings and quick visualizations. Windows‑friendly downloads included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4bc864",
   "metadata": {},
   "source": [
    "> Beginner quick start\n",
    "\n",
    "- Windows: use the Python download cell below (skip the shell one).\n",
    "- Ensure NLTK tokenizers (`punkt`, `punkt_tab`) if tokenization errors appear.\n",
    "- Start with the small Word2Vec subset to keep runs fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc5e915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from adjustText import adjust_text\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b4e654",
   "metadata": {},
   "source": [
    "## Get IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca189b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Windows (optional): shell-based download/extract\n",
    "# !wget https://github.com/SalvatoreRa/tutorial/blob/main/datasets/IMDB.zip?raw=true\n",
    "# !unzip IMDB.zip?raw=true\n",
    "# df = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500c979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_imdb_csv(csv_name='IMDB Dataset.csv', url='https://raw.githubusercontent.com/SalvatoreRa/tutorial/main/datasets/IMDB.zip'):\n",
    "    if os.path.exists(csv_name):\n",
    "        return csv_name\n",
    "    zip_path = 'IMDB.zip'\n",
    "    print(f'Downloading {url} -> {zip_path} ...')\n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "    print(f'Extracting {zip_path} ...')\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        zf.extractall('.')\n",
    "    try:\n",
    "        os.remove(zip_path)\n",
    "    except OSError:\n",
    "        pass\n",
    "    if not os.path.exists(csv_name):\n",
    "        raise FileNotFoundError(f\n",
    "                                )\n",
    "    return csv_name\n",
    "\n",
    "\n",
    "csv_path = ensure_imdb_csv()\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f238ae",
   "metadata": {},
   "source": [
    "## Ensure NLTK tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9188019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    try:\n",
    "        nltk.download('punkt_tab')\n",
    "    except Exception:\n",
    "        local_dir = os.path.join(os.getcwd(), 'nltk_data')\n",
    "        os.makedirs(local_dir, exist_ok=True)\n",
    "        nltk.download('punkt_tab', download_dir=local_dir)\n",
    "        if local_dir not in nltk.data.path:\n",
    "            nltk.data.path.append(local_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123e1d52",
   "metadata": {},
   "source": [
    "## Preprocess + tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a1a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_reviews(reviews):\n",
    "    processed = []\n",
    "    for review in tqdm(reviews):\n",
    "        review = re.sub('<[^>]+>', '', review)\n",
    "        review = re.sub('[^a-zA-Z ]', ' ', review)\n",
    "        words = review.split()\n",
    "        processed.append(' '.join(w.lower() for w in words if len(w) > 1))\n",
    "    return processed\n",
    "\n",
    "\n",
    "df['reviews_processed'] = preprocessing_reviews(df['review'])\n",
    "df['tokens'] = df['reviews_processed'].apply(nltk.word_tokenize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba142dd",
   "metadata": {},
   "source": [
    "## Train Word2Vec (quick subset)\n",
    "- Use a smaller slice to keep runtime short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6525f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "tokens_sample = df['tokens'].head(5000).tolist()\n",
    "model = Word2Vec(sentences=tokens_sample, sg=1,\n",
    "                 vector_size=100, window=5, workers=4)\n",
    "print(f'Time needed on subset: {(time.time()-start_time)/60:.2f} mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0897fa4",
   "metadata": {},
   "source": [
    "## Dendrogram (hierarchical clustering of a few words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea541c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = list(model.wv.index_to_key)\n",
    "highlight_words = ['berlin', 'paris', 'london', 'rome', 'italy',\n",
    "                   'france', 'germany', 'england', 'movie', 'production', 'good', 'bad']\n",
    "hw = [w for w in [h.lower() for h in highlight_words] if w in all_words]\n",
    "vecs = np.array([model.wv[w] for w in hw])\n",
    "linked = linkage(vecs, 'ward')\n",
    "plt.figure(figsize=(6, 4))\n",
    "dendrogram(linked, orientation='top', labels=hw,\n",
    "           distance_sort='descending', show_leaf_counts=True)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0f863e",
   "metadata": {},
   "source": [
    "## Optional: t‑SNE (slow on many points) — cap to top‑N words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1282e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "topN = 2000\n",
    "all_words = list(model.wv.index_to_key)[:topN]\n",
    "all_vecs = np.array([model.wv[w] for w in all_words])\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "Y = tsne.fit_transform(all_vecs)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=Y[:, 0], y=Y[:, 1], s=6, color='steelblue', alpha=.35)\n",
    "plt.title('t-SNE of Word2Vec embeddings (top-N)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117fd223",
   "metadata": {},
   "source": [
    "## Optional: UMAP (often faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff70059",
   "metadata": {},
   "outputs": [],
   "source": [
    "um = UMAP(n_components=2, random_state=42)\n",
    "Y = um.fit_transform(all_vecs)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=Y[:, 0], y=Y[:, 1], s=6, color='darkorange', alpha=.35)\n",
    "plt.title('UMAP of Word2Vec embeddings (top-N)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
