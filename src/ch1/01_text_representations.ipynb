{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "552e70a3",
   "metadata": {},
   "source": [
    "# Text Representations: One-Hot, Bag of Words, TF/IDF\n",
    "\n",
    "Learn simple, non-math ways to turn text into numbers with beginner notes and tiny examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43696bca",
   "metadata": {},
   "source": [
    "> Beginner quick start\n",
    ">\n",
    "> - Run cells from top to bottom. If you change earlier text, re-run later cells.\n",
    "> - Each section has a short note on what to run and how to read the outputs.\n",
    "> - Use the tiny 3-sentence corpus to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851562e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb11b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenize(text: str) -> List[str]:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z]+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf48a3",
   "metadata": {},
   "source": [
    "## One‑Hot Encoding\n",
    "\n",
    "**What it does**\n",
    "Turns each word in a sentence into a list of numbers where exactly one position is “on” (`1`) and all others are “off” (`0`). This simply marks which word it is—no extra meaning or math is added.\n",
    "\n",
    "**Why it’s useful**\n",
    "It’s the most basic way to turn text into numbers so a computer can start processing it.\n",
    "\n",
    "**Example sentence**\n",
    "\n",
    "> “Should we go to a pizzeria or do you prefer a restaurant?”\n",
    "\n",
    "* **Vocabulary** = all unique words in the sentence (sorted).\n",
    "* **One-hot matrix** = rows represent tokens (words in order), columns represent vocabulary; each row has a single `1` in the column for its word.\n",
    "\n",
    "---\n",
    "\n",
    "### How rows and tokens work\n",
    "\n",
    "* **Rows** → words (tokens) in the sentence, in order.\n",
    "* **Number of rows** = number of tokens in your sentence.\n",
    "* **Number of columns** = number of unique words in the vocabulary.\n",
    "\n",
    "### What are “tokens”?\n",
    "\n",
    "* Tokens are the individual words after light cleaning (e.g., lowercase, letters only).\n",
    "* When you run the next cell, you’ll see them printed as:\n",
    "  `Tokens: [...]`\n",
    "\n",
    "### Understanding the one-hot output\n",
    "\n",
    "* Table structure:\n",
    "\n",
    "  * Rows = tokens (words in order)\n",
    "  * Columns = vocabulary (unique words)\n",
    "* Each row has **exactly one** `1` in the column for that word; all other positions are `0`.\n",
    "* The printed shape follows:\n",
    "  `(number_of_tokens, vocabulary_size)`\n",
    "\n",
    "---\n",
    "\n",
    "> **Beginner tips**\n",
    ">\n",
    "> * Run the next cell to see tokens, vocabulary size, and the one-hot table.\n",
    "> * If you change the sentence, re-run the cell to refresh tokens and vocabulary.\n",
    "\n",
    "**Example**\n",
    "\n",
    "Sentence: \"Should we go to a pizzeria or do you prefer a restaurant?\"\n",
    "\n",
    "Vocabulary (unique, sorted): [\"a\", \"do\", \"go\", \"or\", \"pizzeria\", \"prefer\", \"restaurant\", \"should\", \"to\", \"we\", \"you\"]\n",
    "\n",
    "Tokens (in order): [\"should\", \"we\", \"go\", \"to\", \"a\", \"pizzeria\", \"or\", \"do\", \"you\", \"prefer\", \"a\", \"restaurant\"]\n",
    "\n",
    "One‑hot encoding matrix (rows = tokens in order, columns = vocabulary):\n",
    "\n",
    "| Word       | a | do | go | or | pizzeria | prefer | restaurant | should | to | we | you |\n",
    "|------------|---|----|----|----|----------|--------|------------|--------|----|----|-----|\n",
    "| should     | 0 | 0  | 0  | 0  | 0        | 0      | 0          | 1      | 0  | 0  | 0   |\n",
    "| we         | 0 | 0  | 0  | 0  | 0        | 0      | 0          | 0      | 0  | 1  | 0   |\n",
    "| go         | 0 | 0  | 1  | 0  | 0        | 0      | 0          | 0      | 0  | 0  | 0   |\n",
    "| to         | 0 | 0  | 0  | 0  | 0        | 0      | 0          | 0      | 1  | 0  | 0   |\n",
    "| a          | 1 | 0  | 0  | 0  | 0        | 0      | 0          | 0      | 0  | 0  | 0   |\n",
    "| pizzeria   | 0 | 0  | 0  | 0  | 1        | 0      | 0          | 0      | 0  | 0  | 0   |\n",
    "| or         | 0 | 0  | 0  | 1  | 0        | 0      | 0          | 0      | 0  | 0  | 0   |\n",
    "| do         | 0 | 1  | 0  | 0  | 0        | 0      | 0          | 0      | 0  | 0  | 0   |\n",
    "| you        | 0 | 0  | 0  | 0  | 0        | 0      | 0          | 0      | 0  | 0  | 1   |\n",
    "| prefer     | 0 | 0  | 0  | 0  | 0        | 1      | 0          | 0      | 0  | 0  | 0   |\n",
    "| a          | 1 | 0  | 0  | 0  | 0        | 0      | 0          | 0      | 0  | 0  | 0   |\n",
    "| restaurant | 0 | 0  | 0  | 0  | 0        | 0      | 1          | 0      | 0  | 0  | 0   |\n",
    "\n",
    "Note: Column order = Vocabulary order shown above.\n",
    "> Mnemonic: rows = tokens (words in order); columns = vocabulary (unique words).\n",
    "\n",
    "So, for example, \"should\" becomes a vector with a `1` under the \"should\" column and `0`s elsewhere; \"we\" has a `1` under \"we\"; and so on. Duplicate words like \"a\" appear as multiple rows with the same one‑hot pattern.\n",
    "**Mini visual (subset for clarity)**\n",
    "\n",
    "To see the pattern quickly, here’s a tiny subset using just the first three tokens and three vocabulary columns:\n",
    "\n",
    "- Tokens (first 3): [\"should\", \"we\", \"go\"]\n",
    "- Vocabulary (subset): [\"go\", \"should\", \"we\"]\n",
    "\n",
    "| Word   | go | should | we |\n",
    "|--------|----|--------|----|\n",
    "| should | 0  | 1      | 0  |\n",
    "| we     | 0  | 0      | 1  |\n",
    "| go     | 1  | 0      | 0  |\n",
    "\n",
    "Each row has exactly one 1 in the column for that word. The full table below shows all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e8f166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(sentence):\n",
    "    tokens = clean_tokenize(sentence)\n",
    "    vocabulary = sorted(set(tokens))\n",
    "    word_to_index = {w: i for i, w in enumerate(vocabulary)}\n",
    "    mat = np.zeros((len(tokens), len(vocabulary)), dtype=int)\n",
    "    for i, w in enumerate(tokens):\n",
    "        mat[i, word_to_index[w]] = 1\n",
    "    return mat, vocabulary, tokens\n",
    "\n",
    "\n",
    "sentence = 'Should we go to a pizzeria or do you prefer a restaurant?'\n",
    "one_hot_matrix, vocabulary, tokens = one_hot_encoding(sentence)\n",
    "\n",
    "# Display\n",
    "print(f'Sentence: {sentence}')\n",
    "print(f'Tokens: {tokens}')\n",
    "print(f'Vocabulary (Unique, and Sorted): {vocabulary}')\n",
    "\n",
    "# Sizes of Sentence, Tokens, Vocabulary, One-Hot Matrix\n",
    "print(f'Sentence: {len(sentence)}')\n",
    "print(f'Tokens: {len(tokens)}')\n",
    "print(f'Vocabulary size: {len(vocabulary)}')\n",
    "print(f'One-Hot shape: {one_hot_matrix.shape}')\n",
    "\n",
    "print('Note: Column order follows the sorted vocabulary above.')\n",
    "# Pretty table: rows=tokens, cols=vocabulary; highlight 1s\n",
    "# Use a unique index to avoid Styler errors with duplicate labels\n",
    "display_index = [f\"{i+1}. {w}\" for i, w in enumerate(tokens)]\n",
    "one_hot_df = pd.DataFrame(\n",
    "    one_hot_matrix, columns=vocabulary, index=display_index)\n",
    "try:\n",
    "    styled = one_hot_df.style.applymap(\n",
    "        lambda v: 'background-color: #ffeeba' if v == 1 else '')\n",
    "    display(styled)\n",
    "except Exception:\n",
    "    # Fallback if Styler/display not available\n",
    "    one_hot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e848ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own sentence\n",
    "user_sentence = input(\n",
    "    \"Enter a sentence to one-hot encode (or press Enter to reuse the default): \").strip()\n",
    "if user_sentence:\n",
    "    s = user_sentence\n",
    "else:\n",
    "    s = sentence  # reuse the earlier example\n",
    "\n",
    "mat, vocab2, toks2 = one_hot_encoding(s)\n",
    "print('Tokens:', toks2)\n",
    "print(f'Vocab size: {len(vocab2)} | One-Hot shape: {mat.shape}')\n",
    "print('Note: Column order follows the sorted vocabulary.')\n",
    "# Use a unique index to avoid Styler errors with duplicate labels\n",
    "display_index2 = [f\"{i+1}. {w}\" for i, w in enumerate(toks2)]\n",
    "df2 = pd.DataFrame(mat, columns=vocab2, index=display_index2)\n",
    "try:\n",
    "    styled2 = df2.style.applymap(\n",
    "        lambda v: 'background-color: #ffeeba' if v == 1 else '')\n",
    "    display(styled2)\n",
    "except Exception:\n",
    "    df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9966c4",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW)\n",
    "\n",
    "> If you’re focusing only on One‑Hot today, you can stop here. Come back to BoW and TF‑IDF later.\n",
    "\n",
    "What it does:\n",
    "Counts how many times each word appears in each sentence (or document). It builds a vocabulary of unique words and then makes a table of counts.\n",
    "\n",
    "Why it’s useful:\n",
    "Lets us compare sentences by the words they contain and how often. Helpful for search, clustering, or as input features.\n",
    "\n",
    "Tiny example (3 sentences)\n",
    "1) \"This movie is awesome awesome\"\n",
    "2) \"I do not say is good, but neither awesome\"\n",
    "3) \"Awesome? Only a fool can say that\"\n",
    "\n",
    "Vocabulary (sorted): [\"a\", \"awesome\", \"but\", \"can\", \"do\", \"fool\", \"good\", \"i\", \"is\", \"movie\", \"neither\", \"not\", \"only\", \"say\", \"that\", \"this\"]\n",
    "\n",
    "Counts matrix (rows = sentences, columns = vocabulary):\n",
    "| sentence | a | awesome | but | can | do | fool | good | i | is | movie | neither | not | only | say | that | this |\n",
    "|---------:|:-:|:------:|:---:|:---:|:--:|:----:|:----:|:-:|:--:|:-----:|:-------:|:---:|:----:|:---:|:----:|:----:|\n",
    "| 1        | 0 |   2    |  0  |  0  | 0  |  0   |  0   | 0 |  1 |   1   |    0    |  0  |  0   |  0  |  0   |  1   |\n",
    "| 2        | 0 |   1    |  1  |  0  | 1  |  0   |  1   | 1 |  1 |   0   |    1    |  1  |  0   |  1  |  0   |  0   |\n",
    "| 3        | 1 |   1    |  0  |  1  | 0  |  1   |  0   | 0 |  0 |   0   |    0    |  0  |  1   |  1  |  1   |  0   |\n",
    "\n",
    "How to read it\n",
    "- Row 1 has “awesome” twice, and includes “is”, “movie”, and “this” once each.\n",
    "- Row 2 includes single‑occurrence words like “but”, “do”, “good”, “i”, “is”, “neither”, “not”, “say”, and one “awesome”.\n",
    "- Row 3 contains words like “a”, “awesome”, “can”, “fool”, “only”, “say”, “that” each once.\n",
    "\n",
    "Notes\n",
    "- Bag of Words ignores order: “movie awesome” equals “awesome movie”.\n",
    "- It captures counts, not meaning. TF‑IDF and embeddings improve on this.\n",
    "\n",
    "Beginner notes: BoW\n",
    "- Run the next two cells.\n",
    "- The table shows rows=sentences and columns=words; values are counts.\n",
    "- We skip the if-check because the vocab is built from the same tokens we iterate (membership is guaranteed); add it only when using a fixed vocab or handling OOV words.\n",
    "\n",
    "<mark>Each document becomes a vector whose length equals the vocabulary size (often high‑dimensional and sparse) — watch out for the “curse of dimensionality”.</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f732d0f3",
   "metadata": {},
   "source": [
    "> Beginner notes: What to look for in the BoW table\n",
    ">\n",
    "> - Rows = sentences; Columns = vocabulary (unique, sorted)\n",
    "> - Printed shape = (number_of_sentences, vocabulary_size)\n",
    "> - More unique words → longer document vectors (high‑dimensional, often sparse)\n",
    ">\n",
    "> Glossary: see key terms like “sparse vector” and “curse of dimensionality” in ../../docs/terminology.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c9e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(sentences):\n",
    "    tokenized = [clean_tokenize(s) for s in sentences]\n",
    "    print(f\"Tokenized sentences: {tokenized}\")\n",
    "\n",
    "    vocab = sorted(set(w for sent in tokenized for w in sent))\n",
    "    print(f\"Vocabulary: {vocab}\")\n",
    "\n",
    "    w2i = {w: i for i, w in enumerate(vocab)}\n",
    "    print(f\"Word to index mapping: {w2i}\")\n",
    "\n",
    "    mat = np.zeros((len(sentences), len(vocab)), dtype=int)\n",
    "    print(f\"\\nBag of Words matrix shape: {mat.shape}\")\n",
    "    print(f\"Initial Bag of Words matrix:\\n{mat}\")\n",
    "    for i, sent in enumerate(tokenized):\n",
    "        for w in sent:\n",
    "            mat[i, w2i[w]] += 1\n",
    "    return vocab, mat\n",
    "\n",
    "\n",
    "corpus = [\n",
    "    'This movie is awesome awesome',\n",
    "    'I do not say is good, but neither awesome',\n",
    "    'Awesome? Only a fool can say that'\n",
    "]\n",
    "vocab_bow, bow_matrix = bag_of_words(corpus)\n",
    "print('Vocab size:', len(vocab_bow), '| Matrix:', bow_matrix.shape)\n",
    "\n",
    "bow_df = pd.DataFrame(bow_matrix, columns=vocab_bow)\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0027aad9",
   "metadata": {},
   "source": [
    "## TF and TF‑IDF\n",
    "\n",
    "What it does:\n",
    "- TF (Term Frequency): Shows how often each word appears in a sentence, relative to the sentence length.\n",
    "- IDF (Inverse Document Frequency): Downweights words that appear in many sentences; upweights words that are rare across sentences.\n",
    "- TF‑IDF = TF × IDF: Highlights words that are frequent in one sentence but uncommon overall.\n",
    "\n",
    "Why it’s useful:\n",
    "Helps find words that are distinctive to each sentence, useful for search, summarization, and feature engineering.\n",
    "\n",
    "How to read TF and TF‑IDF\n",
    "- TF: Within one sentence, count each word and divide by total words in that sentence (row sums ≈ 1.0).\n",
    "- IDF: Higher for rarer words; equals 1.0 for words present in every sentence (with smoothing).\n",
    "- TF‑IDF: Large when a word is frequent in a sentence and rare across others.\n",
    "\n",
    "Tiny example (same 3 sentences)\n",
    "1) \"this movie is awesome awesome\"\n",
    "2) \"i do not say is good, but neither awesome\"\n",
    "3) \"awesome? only a fool can say that\"\n",
    "\n",
    "Intuition\n",
    "- “awesome” appears in all sentences, so its IDF is lower than a word appearing in only one sentence.\n",
    "- A word that repeats in one sentence can still get a higher TF‑IDF for that sentence.\n",
    "\n",
    "Tiny numeric walk‑through for “awesome”\n",
    "- Sentence lengths: 5, 9, 7 → TF1=2/5=0.40, TF2=1/9≈0.11, TF3=1/7≈0.14\n",
    "- With sklearn‑style smoothing: idf = log((1+N)/(1+df)) + 1 → N=3, df(awesome)=3 → IDF=1.0\n",
    "- TF‑IDF values: 0.40, ~0.11, ~0.14 (highest in sentence 1 due to repetition)\n",
    "\n",
    "Beginner notes: TF/IDF\n",
    "- Run the next cells to compute and view tables.\n",
    "- Larger TF‑IDF means “important for that sentence and uncommon overall”.\n",
    "- Optional: print top TF‑IDF words per sentence to see the standouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34322f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(sentences):\n",
    "    tokenized = [clean_tokenize(s) for s in sentences]\n",
    "    print(f\"Tokenized sentences: {tokenized}\")\n",
    "\n",
    "    vocab = sorted(set(w for sent in tokenized for w in sent))\n",
    "    print(f\"Vocabulary: {vocab}\")\n",
    "\n",
    "    w2i = {w: i for i, w in enumerate(vocab)}\n",
    "    print(f\"Word to index mapping: {w2i}\")\n",
    "\n",
    "    tf = np.zeros((len(sentences), len(vocab)), dtype=np.float32)\n",
    "    print(f\"\\nTF matrix shape: {tf.shape}\")\n",
    "    print(f\"Initial TF matrix:\\n{tf}\")\n",
    "\n",
    "    for i, words in enumerate(tokenized):\n",
    "        n = max(1, len(words))\n",
    "        for w in words:\n",
    "            tf[i, w2i[w]] += 1.0 / n\n",
    "    return tf, vocab\n",
    "\n",
    "\n",
    "def compute_idf(sentences, vocab):\n",
    "    token_sets = [set(clean_tokenize(s)) for s in sentences]\n",
    "    print(f\"Token sets per sentence: {token_sets}\")\n",
    "\n",
    "    N = len(sentences)\n",
    "    w2i = {w: i for i, w in enumerate(vocab)}\n",
    "    print(f\"Word to index mapping: {w2i}\")\n",
    "\n",
    "    idf = np.zeros(len(vocab), dtype=np.float32)\n",
    "    print(f\"\\nIDF vector shape: {idf.shape}\")\n",
    "    print(f\"Initial IDF vector:\\n{idf}\")\n",
    "\n",
    "    for w in vocab:\n",
    "        df = sum(1 for s in token_sets if w in s)\n",
    "        idf[w2i[w]] = np.log((1 + N) / (1 + df)) + \\\n",
    "            1.0  # sklearn-style smoothing\n",
    "    return idf\n",
    "\n",
    "\n",
    "def tf_idf(sentences):\n",
    "    tf, vocab = compute_tf(sentences)\n",
    "    idf = compute_idf(sentences, vocab)\n",
    "    return vocab, tf * idf\n",
    "\n",
    "\n",
    "vocabulary, tf_idf_matrix = tf_idf(corpus)\n",
    "tf_matrix, tf_vocab = compute_tf(corpus)\n",
    "idf_vector = compute_idf(corpus, tf_vocab)\n",
    "print('TF-IDF shape:', tf_idf_matrix.shape)\n",
    "tf_df = pd.DataFrame(tf_matrix, columns=tf_vocab)\n",
    "idf_df = pd.DataFrame([idf_vector], columns=tf_vocab)\n",
    "tfidf_df = pd.DataFrame(tf_idf_matrix, columns=vocabulary)\n",
    "display(tf_df.round(2))\n",
    "display(idf_df.round(2))\n",
    "display(tfidf_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94992f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top TF-IDF terms per sentence\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def top_tfidf_per_doc(vocab, tfidf_matrix, top_k=3):\n",
    "    for i in range(tfidf_matrix.shape[0]):\n",
    "        row = tfidf_matrix[i]\n",
    "        idxs = np.argsort(-row)[:top_k]\n",
    "        pairs = [(vocab[j], float(row[j])) for j in idxs if row[j] > 0]\n",
    "        print(f'Sentence {i+1}: {pairs}')\n",
    "\n",
    "\n",
    "top_tfidf_per_doc(vocabulary, tf_idf_matrix, top_k=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
