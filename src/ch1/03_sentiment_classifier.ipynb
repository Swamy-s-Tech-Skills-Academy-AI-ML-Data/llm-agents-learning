{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8db7d54b",
   "metadata": {},
   "source": [
    "# Sentiment Classifier (GRU)\n",
    "\n",
    "Train a simple GRU-based model to classify IMDB reviews as positive/negative with beginner guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e226648",
   "metadata": {},
   "source": [
    "> Beginner quick start\n",
    "\n",
    "- Windows: use the Python download cell (skip shell).\n",
    "- Start with a small subset and 1 epoch to validate the pipeline, then scale up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09eac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078a7562",
   "metadata": {},
   "source": [
    "## Get IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15596349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Windows (optional): shell-based download/extract\n",
    "# !wget https://github.com/SalvatoreRa/tutorial/blob/main/datasets/IMDB.zip?raw=true\n",
    "# !unzip IMDB.zip?raw=true\n",
    "# df = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe217dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_imdb_csv(csv_name='IMDB Dataset.csv', url='https://raw.githubusercontent.com/SalvatoreRa/tutorial/main/datasets/IMDB.zip'):\n",
    "    if os.path.exists(csv_name):\n",
    "        return csv_name\n",
    "    zip_path = 'IMDB.zip'\n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        zf.extractall('.')\n",
    "    try:\n",
    "        os.remove(zip_path)\n",
    "    except OSError:\n",
    "        pass\n",
    "    if not os.path.exists(csv_name):\n",
    "        raise FileNotFoundError(f\n",
    "                                )\n",
    "    return csv_name\n",
    "\n",
    "\n",
    "csv_path = ensure_imdb_csv()\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8746ff9f",
   "metadata": {},
   "source": [
    "## Quick EDA: word clouds and lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15c2a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def get_words(series):\n",
    "    all_words = []\n",
    "    for review in series:\n",
    "        r = re.sub(r'[^\\w\\s]', ' ', review)\n",
    "        r = re.sub(r'\\d', '', r)\n",
    "        words = r.split()\n",
    "        all_words.extend(\n",
    "            [w for w in words if w not in stop_words and len(w) > 1])\n",
    "    return all_words\n",
    "\n",
    "\n",
    "pos = df[df['sentiment'] == 'positive']['review']\n",
    "neg = df[df['sentiment'] == 'negative']['review']\n",
    "pos_counts = Counter(get_words(pos))\n",
    "neg_counts = Counter(get_words(neg))\n",
    "\n",
    "WordCloud(width=400, height=400, background_color='white').generate_from_frequencies(\n",
    "    pos_counts).to_image()\n",
    "WordCloud(width=400, height=400, background_color='white').generate_from_frequencies(\n",
    "    neg_counts).to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbc707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_lengths(series): return [len(r.split()) for r in series]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(review_lengths(pos), bins=30, color='green', alpha=.7)\n",
    "plt.title('Positive lengths')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(review_lengths(neg), bins=30, color='red', alpha=.7)\n",
    "plt.title('Negative lengths')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2677e5",
   "metadata": {},
   "source": [
    "## Tokenize, vectorize, pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e59253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_review(review):\n",
    "    review = re.sub(r'[^\\w\\s]', ' ', review)\n",
    "    review = re.sub(r'\\s+', ' ', review)\n",
    "    review = re.sub(r'\\d', '', review)\n",
    "    return review.strip().lower()\n",
    "\n",
    "\n",
    "def tokenize_and_build_vocab(reviews, max_vocab=1000):\n",
    "    corpus = Counter()\n",
    "    for r in reviews:\n",
    "        words = word_tokenize(preprocess_review(r))\n",
    "        corpus.update([w for w in words if w not in stop_words and len(w) > 1])\n",
    "    vocab = {w: i+1 for i, (w, _) in enumerate(corpus.most_common(max_vocab))}\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def vectorize_reviews(reviews, vocab):\n",
    "    vec = []\n",
    "    for r in reviews:\n",
    "        words = word_tokenize(preprocess_review(r))\n",
    "        vec.append([vocab[w] for w in words if w in vocab])\n",
    "    return vec\n",
    "\n",
    "\n",
    "X, y = df['review'].values, np.where(df['sentiment'] == 'positive', 0, 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=.2, random_state=42)\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train, y_train, stratify=y_train, test_size=.1, random_state=42)\n",
    "vocab = tokenize_and_build_vocab(x_train, max_vocab=1000)\n",
    "\n",
    "\n",
    "def pad(seqs, max_len):\n",
    "    out = np.zeros((len(seqs), max_len), dtype=int)\n",
    "    for i, s in enumerate(seqs):\n",
    "        out[i, -len(s):] = np.array(s[:max_len]) if len(s) > 0 else 0\n",
    "    return out\n",
    "\n",
    "\n",
    "x_train, x_val, x_test = map(lambda s: pad(\n",
    "    vectorize_reviews(s, vocab), 500), [x_train, x_val, x_test])\n",
    "y_train, y_val, y_test = map(np.array, [y_train, y_val, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fc340e",
   "metadata": {},
   "source": [
    "## Define model (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce21784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, no_layers, vocab_size, hidden_dim, embedding_dim, drop_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.no_layers = no_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(input_size=embedding_dim, hidden_size=hidden_dim,\n",
    "                          num_layers=no_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embeds = self.embedding(x)\n",
    "        rnn_out, hidden = self.rnn(embeds, hidden)\n",
    "        rnn_out = rnn_out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.dropout(rnn_out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sig(out)\n",
    "        out = out.view(x.size(0), -1)[:, -1]\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch):\n",
    "        return torch.zeros((self.no_layers, batch, self.hidden_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d21e3f",
   "metadata": {},
   "source": [
    "## Train (small, quick run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49414e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "no_layers, hidden_dim, embedding_dim = 2, 128, 100\n",
    "model = SentimentRNN(no_layers, len(vocab)+1, hidden_dim,\n",
    "                     embedding_dim).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "train_loader = DataLoader(TensorDataset(torch.from_numpy(\n",
    "    x_train), torch.from_numpy(y_train)), batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(TensorDataset(torch.from_numpy(\n",
    "    x_val), torch.from_numpy(y_val)), batch_size=64)\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    h = model.init_hidden(64).to(device)\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    tr_acc = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device).float()\n",
    "        h = h.data\n",
    "        model.zero_grad()\n",
    "        out, h = model(xb, h)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "        tr_loss += loss.item()*xb.size(0)\n",
    "        preds = (out > 0.5).float()\n",
    "        tr_acc += (preds == yb).sum().item()\n",
    "    print(f'Epoch {epoch+1} Train Loss: {tr_loss/len(train_loader.dataset):.4f} | Train Acc: {tr_acc/len(train_loader.dataset):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738ced27",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3128858",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(TensorDataset(torch.from_numpy(\n",
    "    x_test), torch.from_numpy(y_test)), batch_size=256)\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_true = []\n",
    "with torch.no_grad():\n",
    "    h = model.init_hidden(256).to(device)\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.numpy()\n",
    "        out, h = model(xb, h)\n",
    "        preds = (torch.sigmoid(out) > 0.5).float().cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_true.extend(yb)\n",
    "acc = accuracy_score(all_true, all_preds)\n",
    "cm = confusion_matrix(all_true, all_preds)\n",
    "print('Test accuracy:', acc)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
